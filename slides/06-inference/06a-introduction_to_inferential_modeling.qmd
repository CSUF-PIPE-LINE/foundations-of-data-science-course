---
title: "Introduction to Inferential Modeling"
format: revealjs
editor: visual
execute:
  echo: TRUE
---

## Steps 1 and 2: Load Packages and Import Data

```{r}
#| label: load packages 
library(tidyverse) 
```

```{r}
#| label: import data with here
alz <- readr::read_csv(here::here("Data", "alzheimers.csv"))
```

## Inferential Modeling

-   Inferential models build a mechanism for describing the effect of predictors on the response

$$
\text{DATA} = \text{MODEL} + \text{ERROR}
$$

-   **MODEL**: a mathematical function of parameters ($\theta$) and predictor variables (x)

## Simple Linear Regression

$$
y = \beta_0 + \beta_1 x + \text{ERROR}
$$

-   In inferential modeling, we would like to:

    -   Determine whether the $\beta_1 x$ term is necessary

    -   Estimate the true value of $\beta_1$

## Making Assumptions

$$
y = \beta_0 + \beta_1 x + \text{ERROR}
$$

-   To do inferential modeling, we make assumptions about the **MODEL** and the **ERROR**

-   Under those assumptions, we estimate the "true" **MODEL** and **ERROR**  using our sample data

    -   We estimate $\beta_0$ and $\beta_1$ using our fitted $\hat{\beta}_0$ and $\hat{\beta}_1$
    
    -   We estimate the **ERROR** term using the residuals

## Model Assumptions Are Never Met!

-   Real data *never* matches our assumptions perfectly

    -   Even if we *know* the **MODEL** and **ERROR** exactly, the assumptions will not be met *exactly* in our sample data

-   If modeling assumptions look "close to met," we can still use inferential modeling

-   Our goal is to determine which assumptions, if any, are "obviously wrong"

    
## Model Assumptions

-   **Linearity**: the true relationship between $x$ and $y$ is linear plus the ERROR term

::: columns
::: {.column width="50%"}
We expect something like:

```{r}
#| label: lm-linearity-fake
#| echo: FALSE
#| eval: TRUE
#| fig-height: 8
alz_lm <- lm(gmv ~ age, data = alz)
set.seed(196)
alz_norm_resid <- rnorm(nrow(alz), 0, sd(alz_lm$residuals))

alz_fake <- data.frame(
  age = alz$age,
  resid = alz_norm_resid,
  pred_gmv = predict(alz_lm),
  gmv = predict(alz_lm) + alz_norm_resid
)

ggplot(data = alz_fake, 
       mapping = aes(x = age, 
                     y = gmv)) +
  geom_point(color = "navy", size = 2) +
  geom_smooth(method = "lm", color = "black", 
              se = FALSE) +
  labs(x = "Age (years)",
       y = "Gray Matter Volume (cc)") +
  theme(
    axis.title = element_text(size = 24),
    axis.text = element_text(size = 16)
  )
```
:::

::: {.column width="50%"}
We actually see:

```{r}
#| label: lm-linearity-real
#| echo: FALSE
#| eval: TRUE
#| fig-height: 8

alz_real <- data.frame(
  age = alz$age,
  gmv = alz$gmv,
  resid = alz_lm$residuals,
  pred_gmv = predict(alz_lm)
)

ggplot(data = alz_real, 
       mapping = aes(x = age, 
                     y = gmv)) +
  geom_point(color = "navy", size = 2) +
  geom_smooth(method = "lm", color = "black", 
              se = FALSE) +
  labs(x = "Age (years)",
       y = "Gray Matter Volume (cc)") +
  theme(
    axis.title = element_text(size = 24),
    axis.text = element_text(size = 16)
  )
```
:::
:::

## Model Assumptions

-   **Zero Mean Error**: the ERROR term is centered at 0 independently of the model predictions

::: columns
::: {.column width="50%"}
We expect something like:

```{r}
#| label: lm-zeromean-fake
#| echo: FALSE
#| eval: TRUE
#| fig-height: 8

ggplot(data = alz_fake, 
       mapping = aes(x = pred_gmv, 
                     y = resid)) +
  geom_point(color = "red", size = 2) +
  geom_smooth(method = "loess", color = "black", 
              se = FALSE) +
  labs(x = "Predicted gmv (cc)",
       y = "Residual") +
  theme(
    axis.title = element_text(size = 24),
    axis.text = element_text(size = 16)
  )
```
:::

::: {.column width="50%"}
We actually see:

```{r}
#| label: lm-zeromean-real
#| echo: FALSE
#| eval: TRUE
#| fig-height: 8

ggplot(data = alz_real, 
       mapping = aes(x = pred_gmv, 
                     y = resid)) +
  geom_point(color = "red", size = 2) +
  geom_smooth(method = "loess", color = "black", 
              se = FALSE) +
  labs(x = "Predicted gmv (cc)",
       y = "Residual") +
  theme(
    axis.title = element_text(size = 24),
    axis.text = element_text(size = 16)
  )
```
:::
:::

## Model Assumptions

-   **Constant Variation Error**: the ERROR term has consistent variability across the entire range of model predictions

::: columns
::: {.column width="50%"}
We expect something like:

```{r}
#| label: lm-homoskedasticity-fake
#| echo: FALSE
#| eval: TRUE
#| fig-height: 8

ggplot(data = alz_fake, 
       mapping = aes(x = pred_gmv, 
                     y = sqrt(abs(resid)))) +
  geom_point(color = "red", size = 2) +
  geom_smooth(method = "loess", color = "black", 
              se = FALSE) +
  labs(x = "Predicted gmv (cc)",
       y = "Square Root of |Residual|") +
  theme(
    axis.title = element_text(size = 24),
    axis.text = element_text(size = 16)
  )
```
:::

::: {.column width="50%"}
We actually see:

```{r}
#| label: lm-heteroskedasticity-real
#| echo: FALSE
#| eval: TRUE
#| fig-height: 8

ggplot(data = alz_real, 
       mapping = aes(x = pred_gmv, 
                     y = sqrt(abs(resid)))) +
  geom_point(color = "red", size = 2) +
  geom_smooth(method = "loess", color = "black", 
              se = FALSE) +
  labs(x = "Predicted gmv (cc)",
       y = "Square Root of |Residual|") +
  theme(
    axis.title = element_text(size = 24),
    axis.text = element_text(size = 16)
  )
```
:::
:::

## Model Assumptions

-   **Normally Distributed Error**: the ERROR term has a normal distribution

::: columns
::: {.column width="50%"}
We expect something like:

```{r}
#| label: lm-normality-fake
#| echo: FALSE
#| eval: TRUE
#| fig-height: 8

alz_density <- data.frame(
  x = seq(-200, 200, by = 1)
) |>
  mutate(
  y = dnorm(x, 0, sd(alz_lm$residuals))
)

ggplot(data = alz_fake, 
       mapping = aes(x = resid)) +
  geom_histogram(
    center = 0,
    binwidth = 10,
    fill = "red") +
  geom_line(
    data = alz_density,
    mapping = aes(x = x, y = y*850),
    size = 2
  ) +
  labs(x = "Residual",
       y = "Number of Residuals") +
  theme(
    axis.title = element_text(size = 24),
    axis.text = element_text(size = 16)
  )
```
:::

::: {.column width="50%"}
We actually see:

```{r}
#| label: lm-normality-real
#| echo: FALSE
#| eval: TRUE
#| fig-height: 8

ggplot(data = alz_real, 
       mapping = aes(x = resid)) +
  geom_histogram(
    center = 0,
    binwidth = 10,
    fill = "red") +
  geom_line(
    data = alz_density,
    mapping = aes(x = x, y = y*850),
    size = 2
  ) +
  labs(x = "Residual",
       y = "Number of Residuals") +
  theme(
    axis.title = element_text(size = 24),
    axis.text = element_text(size = 16)
  )
```
:::
:::

## Model Assumptions

-   **Independence**: the ERROR terms for each observation are independent

    -   This one we have to check based on our understanding of the data

    -   Violated if multiple points on the scatterplot come from extremely similar observations

    -   Generally violated if observations are closely linked in space or time

## What Do We Do If Assumptions Are Violated?

-   Use EDA to investigate reasonable *nonlinear* relationships

-   Transform $x$ or $y$ or both

    -   Logarithmic or square root transforms are popular when the variable is highly skewed right
    
    -   Many flexible options for piecewise functions of $x$
    
-   Use inferential methods with fewer assumptions
