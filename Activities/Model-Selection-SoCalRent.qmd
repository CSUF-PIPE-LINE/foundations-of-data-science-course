---
title: "Model Selection for Regression: Rent Prices in Southern California"
author: "Your Name Here!"
format: html
editor: visual
---

## Load Packages, Import Data, Massage Data

```{r}
#| label: do this first
here::i_am("Activities/Model-Selection-SoCalRent.qmd")

library(tidyverse)
library(broom)
library(rsample)
library(yardstick)

SoCalRent1 <- read_csv(here::here("Data/SoCalRent1.csv"))
SoCalRent2 <- read_csv(here::here("Data/SoCalRent2.csv"))

```

In this activity we are going to try to predict the rent price from information about the house.

In the "Summarizing Linear Models" activity, we removed some of the houses from this dataset. Repeat that now:

```{r}
#| label: create SoCalRent_train


```

## Setting Up our Cross-Validation

Remember that we have already partitioned our houses into the training and test sets.

We create our folds using the training set. We'll use 10-fold cross-validation, which puts about 18 observations in each of the folds.

```{r}
#| label: v-fold cv setup

```

## Fitting the Models

Next, we create our function that's going to fit the model on all but one fold and make predictions on the other fold.

Here we're going to make our prediction based only on a single predictor: `SqFt`

```{r}
#| label: function for prediction

```

Now we map the predictions over all the folds:

```{r}
#| label: map the predictions

```

Then we look at how well-calibrated the model is:

```{r}
#| label: predicted vs observed plot

```

```{r}
#| label: residual vs. fit plot

```

After looking at our calibration, we can evaluate the model by RMSE:

```{r}
#| label: look at RMSE


```

### Your Turn

1.  How well-calibrated is our model? Are there some median values that our model is having difficulty predicting at all accurately?

2.  Inside the `SoCalRent_prediction` function, fit four more multiple linear regression models. Three of them can use any combination of predictors (`SqFt`, `Beds`, `Baths`, `Type`, `Location`) you want, but the fourth should be the null model. Modify the function so that `valid_predictions` contains the predictions from all five models.

Note that we really *should* be deciding on these models based on our exploratory data analysis; however, here our focus is on learning model selection, so we'll just make some models to select from.

3.  Re-run the chunk with the `SoCalRent_prediction` function, then re-run the chunk creating `mapped_predictions_df`. Produce a calibration and/or residual vs. fit plot for the other 4 models.

4.  Do any of your models appear to be noticeably better calibrated than the other(s)? Why or why not?

5.  Rewrite the "look at RMSE" chunk to get the average cross-validated RMSE and its standard error for each of the five models. It may be easiest to first use `pivot_longer` followed by `group_by` and `rmse` to get the RMSE on each fold for each model.

6.  Which model appears to be making the best predictions on new data? Why?

The "one-standard-error" rule of thumb suggests to find the model with the minimum average RMSE and add the standard error of that mean to get an "RMSE bound." All models with RMSE under that bound are considered to be competitive, and we almost always select the simplest competitive model.

7.  Using this "one-standard-error" rule of thumb, would you select a different model than you selected in Question 6? If so, which one would you select instead and why? If not, why not?

## Making Final Predictions

### Your Turn

1.  For the model you selected in the previous section, re-fit it on the *full* training set, and make predictions on the test set.

2.  Obtain the RMSE on the test set. Compare this value to the RMSE you obtained in the previous section.
